# NLP_Word-Embedding:
Word Embedding is a technique used in Natural Language Processing (NLP) to represent words as dense, fixed-size vectors of real numbers. These embeddings capture semantic relationships between words, such that words with similar meanings have similar vector representations. Word embeddings are a significant improvement over traditional techniques like Bag of Words (BOW) or TF-IDF because they can capture context, relationships, and similarities.
It is deep learning technique. # CBOW, Word2vec, Glove, fastText

BOW,TF_IDF and one-hot encoding . It is count or frequency technique

# Word2Vec:

![image](https://github.com/user-attachments/assets/7bad3085-a10e-4a7a-88cc-54596407b8f2)

# Glove
![image](https://github.com/user-attachments/assets/03ad5ca1-4823-47b3-8ed2-07e8325322a6)

# CBOW(Continuous Bag of words)

![image](https://github.com/user-attachments/assets/563da16d-71ed-4de9-bf2f-1a4cc4bd6733)

# Avg Word2vec:
Average Word2Vec is a simple and effective technique for representing entire sentences, paragraphs, or documents using Word2Vec embeddings. It computes the average of the Word2Vec embeddings of all words in the text to create a single fixed-length vector. This representation captures the general semantic meaning of the text while being computationally inexpensive.

Add each and every model in row to get the AVG.



